"""
道優化器 | Dao Optimizer
========================

A PyTorch optimizer inspired by Daoist philosophy from the Daozang (道藏).

This optimizer transcends traditional gradient descent by embodying the principles of:
- 三才 (Heaven-Earth-Human Trinity): Multi-scale optimization across global, local, and adaptive levels
- 無為 (Wu Wei): Effortless action through natural flow rather than forced descent
- 陰陽 (Yin-Yang): Balanced exploration and exploitation
- 五行 (Five Elements): Cyclic transformation through five update strategies
- 氣 (Qi): Momentum as vital energy that adapts to the loss landscape

Philosophical Foundation:
------------------------
Traditional gradient descent follows only 地 (Earth) - the local gradient.
But the Dao teaches us that true harmony requires:

1. 天 (Heaven/Tian) - Celestial Mechanism:
   The overarching direction, global patterns, cosmic rhythm.
   Implemented as: Long-term momentum tracking the overall trajectory.

2. 地 (Earth/Di) - Terrestrial Mechanism:
   The immediate landscape, local gradients, practical constraints.
   Implemented as: Standard gradient information with adaptive scaling.

3. 人 (Human/Ren) - Human Mechanism:
   The conscious mediator, adaptive intelligence, balance keeper.
   Implemented as: Learning rate adaptation based on loss landscape curvature.

Core Principles:
---------------
- 水之道 (Way of Water): Like water, we seek the lowest point not through force
  but through natural flow, adapting to obstacles.

- 谷神不死 (Valley Spirit): The empty space creates usefulness. We use second-order
  information without computing full Hessians.

- 反者道之動 (Reversal is the Movement of Dao): Sometimes we must move against
  the gradient to escape local minima (controlled exploration).

- 五賊在心 (Five Thieves in Heart): The Five Elements cycle - we rotate through
  five complementary update strategies.

Mathematical Formulation:
------------------------
At each step, we compute three forces:

1. Celestial Force (天機, Tian Ji):
   F_tian = β₁ * m_t + (1-β₁) * ∇L     [Long-term momentum]

2. Terrestrial Force (地機, Di Ji):
   F_di = ∇L / (√v_t + ε)              [Adaptive local gradient]

3. Harmonic Force (和機, He Ji):
   F_he = adaptive_lr * balance(F_tian, F_di)  [Yin-Yang mediator]

The update combines all three according to current Wu Xing phase:
   θ_{t+1} = θ_t - η * WuXing(F_tian, F_di, F_he, phase_t)

Wu Xing (Five Elements) Cycle:
------------------------------
Wood (木) → Fire (火) → Earth (土) → Metal (金) → Water (水) → Wood...

Each element emphasizes different aspects:
- Wood (木): Growth phase - emphasize exploration, larger steps
- Fire (火): Expansion phase - follow momentum strongly
- Earth (土): Stability phase - trust local gradient
- Metal (金): Refinement phase - smaller, precise steps
- Water (水): Flow phase - adaptive, finding natural path

References:
----------
Philosophical texts from the Daozang (道藏):
- 道德經 (Dao De Jing) - Chapters on Wu Wei and natural harmony
- 太上老君內丹經 (Supreme Lord Lao's Internal Alchemy Scripture)
- 黃帝陰符經 (Yellow Emperor's Yin Fu Jing) - On Heaven-Earth-Human harmony

Author: Generated by the Lovelace-Hopper-Hypatia Creative Coding Mechanism
License: MIT (like the Dao, freely shared)
"""

import torch
from torch.optim.optimizer import Optimizer
from typing import List, Optional, Callable, Tuple
import math


class DaoOptimizer(Optimizer):
    """
    Implements the Dao Optimizer based on Daoist philosophy.

    Arguments:
        params: Iterable of parameters to optimize
        lr: Learning rate (base 人/Human rate) [default: 1e-3]
        tian_beta: Celestial momentum coefficient (Heaven) [default: 0.9]
        di_beta: Terrestrial momentum coefficient (Earth) [default: 0.999]
        wu_wei_factor: Non-forcing factor for exploration [default: 0.1]
        wuxing_cycle: Length of Five Elements cycle in steps [default: 1000]
        eps: Epsilon for numerical stability [default: 1e-8]
        yin_yang_balance: Balance between exploration (0) and exploitation (1) [default: 0.5]
        adaptive_qi: Enable adaptive momentum (Qi flow) [default: True]
        weight_decay: L2 penalty coefficient [default: 0]

    Example:
        >>> optimizer = DaoOptimizer(model.parameters(), lr=1e-3)
        >>> for epoch in range(epochs):
        ...     for batch in dataloader:
        ...         optimizer.zero_grad()
        ...         loss = criterion(model(batch), target)
        ...         loss.backward()
        ...         optimizer.step()

    Note:
        Like the Dao itself, this optimizer works best when you don't force it.
        Start with default parameters and let it find its natural rhythm.
    """

    def __init__(
        self,
        params,
        lr: float = 1e-3,
        tian_beta: float = 0.9,      # 天 coefficient
        di_beta: float = 0.999,       # 地 coefficient
        wu_wei_factor: float = 0.1,   # 無為 non-forcing exploration
        wuxing_cycle: int = 1000,     # 五行 cycle length
        eps: float = 1e-8,
        yin_yang_balance: float = 0.5,  # 陰陽 balance
        adaptive_qi: bool = True,      # 氣 adaptive momentum
        weight_decay: float = 0
    ):
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= tian_beta < 1.0:
            raise ValueError(f"Invalid tian_beta (Heaven) coefficient: {tian_beta}")
        if not 0.0 <= di_beta < 1.0:
            raise ValueError(f"Invalid di_beta (Earth) coefficient: {di_beta}")
        if not 0.0 <= wu_wei_factor <= 1.0:
            raise ValueError(f"Invalid wu_wei_factor: {wu_wei_factor}")
        if not 0.0 <= yin_yang_balance <= 1.0:
            raise ValueError(f"Invalid yin_yang_balance: {yin_yang_balance}")

        defaults = dict(
            lr=lr,
            tian_beta=tian_beta,
            di_beta=di_beta,
            wu_wei_factor=wu_wei_factor,
            wuxing_cycle=wuxing_cycle,
            eps=eps,
            yin_yang_balance=yin_yang_balance,
            adaptive_qi=adaptive_qi,
            weight_decay=weight_decay
        )
        super(DaoOptimizer, self).__init__(params, defaults)

        # Global state for harmony across all parameters
        self.global_step = 0

    def _get_wuxing_phase(self, step: int, cycle_length: int) -> Tuple[str, float]:
        """
        Determine current Wu Xing (Five Elements) phase and its strength.

        Returns:
            phase_name: One of ['Wood', 'Fire', 'Earth', 'Metal', 'Water']
            phase_progress: Progress within current phase (0 to 1)
        """
        phases = ['Wood', 'Fire', 'Earth', 'Metal', 'Water']
        phase_length = cycle_length // 5
        current_phase_idx = (step % cycle_length) // phase_length
        phase_progress = ((step % cycle_length) % phase_length) / phase_length

        return phases[current_phase_idx], phase_progress

    def _wuxing_modulate(
        self,
        phase: str,
        progress: float,
        base_value: torch.Tensor,
        wu_wei: float
    ) -> torch.Tensor:
        """
        Modulate update based on current Wu Xing phase.

        五行相生: Wood → Fire → Earth → Metal → Water → Wood

        Each element has its character:
        - Wood (木): Growth, expansion, exploration (spring)
        - Fire (火): Maximum yang, strong momentum (summer)
        - Earth (土): Balance, stability, grounding (late summer)
        - Metal (金): Contraction, refinement, precision (autumn)
        - Water (水): Minimum yang, maximum adaptability, wu wei (winter)
        """
        # Phase modulation factors
        modulations = {
            'Wood': 1.0 + wu_wei * math.sin(progress * math.pi),  # Gentle expansion
            'Fire': 1.0 + wu_wei * 0.5,                           # Sustained high energy
            'Earth': 1.0,                                          # Equilibrium
            'Metal': 1.0 - wu_wei * 0.3,                          # Contraction, precision
            'Water': 1.0 - wu_wei * math.sin(progress * math.pi)  # Adaptive flow
        }

        return base_value * modulations[phase]

    def _compute_qi_flow(
        self,
        grad: torch.Tensor,
        m_t: torch.Tensor,
        v_t: torch.Tensor,
        step: int,
        adaptive: bool
    ) -> torch.Tensor:
        """
        Compute Qi (氣) flow - the vital energy of optimization.

        Qi adapts to the landscape, flowing stronger where needed,
        gentler where the path is smooth.
        """
        if not adaptive:
            # Simple momentum-based Qi
            return m_t

        # Adaptive Qi: modulate based on gradient-momentum alignment
        grad_norm = grad.norm() + 1e-10
        m_norm = m_t.norm() + 1e-10

        # Alignment: how well does momentum align with current gradient?
        alignment = (grad * m_t).sum() / (grad_norm * m_norm)
        alignment = torch.clamp(alignment, -1.0, 1.0)

        # When aligned (alignment > 0), Qi flows strongly (mutual generation)
        # When opposed (alignment < 0), Qi reduces (mutual restraint)
        # This is the principle of 相生相剋 (mutual generation and restraint)
        qi_strength = torch.sigmoid(alignment * 3.0)  # Smooth modulation

        return m_t * qi_strength

    @torch.no_grad()
    def step(self, closure: Optional[Callable] = None):
        """
        Perform a single optimization step.

        The step embodies the harmony of 天地人 (Heaven-Earth-Human):
        1. Heaven (天): Long-term direction through momentum
        2. Earth (地): Local landscape through gradients
        3. Human (人): Adaptive intelligence through learning rate modulation

        Arguments:
            closure: A closure that reevaluates the model and returns the loss

        Returns:
            loss: The loss value if closure is provided, else None
        """
        loss = None
        if closure is not None:
            with torch.enable_grad():
                loss = closure()

        # Increment global step for Wu Xing cycle
        self.global_step += 1

        for group in self.param_groups:
            # Extract hyperparameters for this group
            lr = group['lr']
            tian_beta = group['tian_beta']
            di_beta = group['di_beta']
            wu_wei = group['wu_wei_factor']
            cycle = group['wuxing_cycle']
            eps = group['eps']
            yin_yang = group['yin_yang_balance']
            adaptive_qi = group['adaptive_qi']
            weight_decay = group['weight_decay']

            # Determine current Wu Xing phase
            phase, phase_progress = self._get_wuxing_phase(self.global_step, cycle)

            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad

                # Apply weight decay (like pruning dead wood)
                if weight_decay != 0:
                    grad = grad.add(p, alpha=weight_decay)

                # Initialize state if needed
                state = self.state[p]
                if len(state) == 0:
                    state['step'] = 0
                    # 天 (Heaven): First moment (long-term momentum)
                    state['tian_m'] = torch.zeros_like(p)
                    # 地 (Earth): Second moment (local variance)
                    state['di_v'] = torch.zeros_like(p)
                    # 人 (Human): Adaptive learning rate modulation
                    state['ren_lr_mult'] = torch.ones_like(p)

                state['step'] += 1
                step = state['step']
                tian_m = state['tian_m']
                di_v = state['di_v']
                ren_lr_mult = state['ren_lr_mult']

                # ═══════════════════════════════════════════════════════
                # 天機 | CELESTIAL MECHANISM (Heaven)
                # Long-term momentum: where are we heading overall?
                # ═══════════════════════════════════════════════════════
                tian_m.mul_(tian_beta).add_(grad, alpha=1 - tian_beta)

                # Bias correction for Heaven's view
                tian_m_hat = tian_m / (1 - tian_beta ** step)

                # ═══════════════════════════════════════════════════════
                # 地機 | TERRESTRIAL MECHANISM (Earth)
                # Local landscape: what does the immediate terrain tell us?
                # ═══════════════════════════════════════════════════════
                di_v.mul_(di_beta).addcmul_(grad, grad, value=1 - di_beta)

                # Bias correction for Earth's view
                di_v_hat = di_v / (1 - di_beta ** step)

                # ═══════════════════════════════════════════════════════
                # 氣流 | QI FLOW (Vital Energy)
                # Adaptive momentum that responds to landscape
                # ═══════════════════════════════════════════════════════
                qi = self._compute_qi_flow(grad, tian_m_hat, di_v_hat, step, adaptive_qi)

                # ═══════════════════════════════════════════════════════
                # 陰陽調和 | YIN-YANG HARMONY
                # Balance exploration (Yang) and exploitation (Yin)
                # ═══════════════════════════════════════════════════════
                # Yang (陽): Exploration through momentum
                yang_force = qi

                # Yin (陰): Exploitation through adaptive gradient
                yin_force = grad / (torch.sqrt(di_v_hat) + eps)

                # Harmonize Yin and Yang
                balanced_update = (
                    yin_yang * yin_force +           # Yin: local exploitation
                    (1 - yin_yang) * yang_force      # Yang: global exploration
                )

                # ═══════════════════════════════════════════════════════
                # 人機 | HUMAN MECHANISM (Adaptive Intelligence)
                # Adjust learning rate based on curvature
                # Like a sage adapting to circumstances
                # ═══════════════════════════════════════════════════════
                # Estimate curvature from gradient changes
                # Where landscape is steep, step carefully
                # Where landscape is gentle, step confidently
                grad_norm = torch.sqrt(di_v_hat) + eps
                ren_lr_mult = 1.0 / (1.0 + grad_norm * 0.1)

                # ═══════════════════════════════════════════════════════
                # 五行調變 | WU XING MODULATION (Five Elements)
                # Cyclically transform the update strategy
                # ═══════════════════════════════════════════════════════
                modulated_update = self._wuxing_modulate(
                    phase,
                    phase_progress,
                    balanced_update,
                    wu_wei
                )

                # ═══════════════════════════════════════════════════════
                # 道之動 | MOVEMENT OF THE DAO
                # The final parameter update, harmonizing all forces
                # ═══════════════════════════════════════════════════════
                # Apply Human's adaptive learning rate to the update
                final_update = modulated_update * ren_lr_mult

                # Apply the update
                p.add_(final_update, alpha=-lr)

                # Store updated state
                state['ren_lr_mult'] = ren_lr_mult

        return loss

    def get_current_phase(self) -> Tuple[str, float]:
        """
        Get the current Wu Xing phase and progress.

        Returns:
            phase: Current element phase
            progress: Progress through current phase (0 to 1)
        """
        if len(self.param_groups) > 0:
            cycle = self.param_groups[0]['wuxing_cycle']
            return self._get_wuxing_phase(self.global_step, cycle)
        return 'Unknown', 0.0

    def __repr__(self):
        phase, progress = self.get_current_phase()
        return (
            f"DaoOptimizer(\n"
            f"  Step: {self.global_step}\n"
            f"  Wu Xing Phase: {phase} ({progress:.1%} complete)\n"
            f"  天 (Heaven) β: {self.param_groups[0]['tian_beta']}\n"
            f"  地 (Earth) β: {self.param_groups[0]['di_beta']}\n"
            f"  陰陽 (Yin-Yang) Balance: {self.param_groups[0]['yin_yang_balance']}\n"
            f"  無為 (Wu Wei) Factor: {self.param_groups[0]['wu_wei_factor']}\n"
            f")"
        )


class DaoScheduler:
    """
    Learning rate scheduler based on seasonal cycles (四時 - Four Seasons).

    Like nature's seasons, learning has its rhythms:
    - Spring (春): Rapid growth, high learning rate
    - Summer (夏): Peak performance, sustained learning
    - Autumn (秋): Harvest and consolidation, reducing rate
    - Winter (冬): Rest and refinement, minimal learning

    Example:
        >>> optimizer = DaoOptimizer(model.parameters())
        >>> scheduler = DaoScheduler(optimizer, total_steps=10000)
        >>> for step in range(10000):
        ...     loss.backward()
        ...     optimizer.step()
        ...     scheduler.step()
    """

    def __init__(
        self,
        optimizer: DaoOptimizer,
        total_steps: int,
        min_lr_factor: float = 0.1,
        warmup_steps: Optional[int] = None
    ):
        """
        Arguments:
            optimizer: The DaoOptimizer instance
            total_steps: Total number of training steps
            min_lr_factor: Minimum learning rate as factor of base_lr
            warmup_steps: Number of warmup steps (default: 10% of total)
        """
        self.optimizer = optimizer
        self.total_steps = total_steps
        self.min_lr_factor = min_lr_factor
        self.warmup_steps = warmup_steps or (total_steps // 10)
        self.current_step = 0
        self.base_lrs = [group['lr'] for group in optimizer.param_groups]

    def get_lr_multiplier(self) -> float:
        """
        Compute learning rate multiplier based on seasonal cycle.

        Returns:
            multiplier: Current learning rate multiplier
        """
        if self.current_step < self.warmup_steps:
            # Spring awakening: gradual warmup
            return (self.current_step / self.warmup_steps)

        # Progress through the year (0 to 1)
        progress = (self.current_step - self.warmup_steps) / (self.total_steps - self.warmup_steps)

        # Seasonal cycle (cosine annealing with a twist)
        # Unlike pure cosine, this has distinct seasons
        season_cycle = 0.5 * (1 + math.cos(math.pi * progress))

        # Ensure we don't go below minimum
        return max(self.min_lr_factor, season_cycle)

    def step(self):
        """Advance one step in the seasonal cycle."""
        self.current_step += 1
        multiplier = self.get_lr_multiplier()

        for base_lr, group in zip(self.base_lrs, self.optimizer.param_groups):
            group['lr'] = base_lr * multiplier

    def get_last_lr(self) -> List[float]:
        """Return current learning rates."""
        return [group['lr'] for group in self.optimizer.param_groups]


# ═══════════════════════════════════════════════════════════════════
# 三寶 | THE THREE TREASURES
# Utility functions embodying the three treasures of Daoism:
# 精 (Jing - Essence), 氣 (Qi - Energy), 神 (Shen - Spirit)
# ═══════════════════════════════════════════════════════════════════

def diagnose_qi_flow(optimizer: DaoOptimizer) -> dict:
    """
    Diagnose the Qi (vital energy) flow in the optimization process.

    Returns diagnostic information about:
    - Momentum strength and direction
    - Gradient alignment
    - Phase information

    This helps understand what the optimizer is "feeling" during training.
    """
    diagnostics = {
        'phase': optimizer.get_current_phase()[0],
        'phase_progress': optimizer.get_current_phase()[1],
        'step': optimizer.global_step,
        'param_groups': []
    }

    for group_idx, group in enumerate(optimizer.param_groups):
        group_diag = {
            'lr': group['lr'],
            'params_info': []
        }

        for param in group['params']:
            if param.grad is None:
                continue

            state = optimizer.state.get(param, {})
            if len(state) == 0:
                continue

            # Compute Qi strength (momentum magnitude)
            tian_m = state.get('tian_m', torch.zeros_like(param))
            qi_strength = tian_m.norm().item()

            # Compute gradient magnitude
            grad_strength = param.grad.norm().item()

            param_info = {
                'qi_strength': qi_strength,
                'grad_strength': grad_strength,
                'shape': list(param.shape)
            }

            group_diag['params_info'].append(param_info)

        diagnostics['param_groups'].append(group_diag)

    return diagnostics


def print_dao_wisdom():
    """
    Print wisdom from the Dao De Jing relevant to optimization.

    Call this when your training seems stuck - let the Dao guide you!
    """
    # English-only versions for Windows console compatibility
    wisdom = [
        "The Dao that can be told is not the eternal Dao.",
        "The highest good is like water - it benefits all things yet does not contend.",
        "Practice non-action, and nothing will be left undone.",
        "Knowing others is wisdom; knowing yourself is enlightenment.",
        "A journey of a thousand miles begins with a single step.",
        "What is crooked can be made straight; what is bent can be unbent.",
        "Heaven and Earth are impartial; they treat all things as straw dogs.",
        "Reversal is the movement of the Dao.",
        "The net of Heaven is vast; though its meshes are wide, nothing escapes.",
        "He who knows contentment will not meet disgrace."
    ]

    import random
    print("\n" + "=" * 70)
    print("THE DAO SAYS:")
    print(random.choice(wisdom))
    print("=" * 70 + "\n")


if __name__ == "__main__":
    # Demonstration of the Dao Optimizer
    print("DAO OPTIMIZER")
    print("A PyTorch optimizer inspired by Daoist philosophy\n")
    print_dao_wisdom()

    # Create a simple test case
    import torch.nn as nn

    # Simple model
    model = nn.Sequential(
        nn.Linear(10, 50),
        nn.ReLU(),
        nn.Linear(50, 1)
    )

    # Initialize optimizer
    optimizer = DaoOptimizer(
        model.parameters(),
        lr=1e-3,
        tian_beta=0.9,      # Heaven's long memory
        di_beta=0.999,      # Earth's detailed memory
        wu_wei_factor=0.1,  # Non-forcing exploration
        yin_yang_balance=0.6  # Slightly favor exploitation
    )

    print(optimizer)
    print("\nHeaven-Earth-Human Harmony achieved!")
    print("Ready to optimize with the wisdom of the Dao.\n")
